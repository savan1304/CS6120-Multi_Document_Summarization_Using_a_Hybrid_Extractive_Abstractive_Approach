{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/parmar.sa/.venv/lib/python3.9/site-packages (0.5.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /home/parmar.sa/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /home/parmar.sa/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m13 packages\u001b[0m \u001b[2min 42ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary packages\n",
    "%pip install uv\n",
    "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!uv pip install nltk datasets spacy evaluate transformers rouge-score matplotlib sympy scipy accelerate scikit-learn wandb bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Successfully downloaded punkt\n",
      "Successfully downloaded stopwords\n",
      "Successfully downloaded averaged_perceptron_tagger\n",
      "Successfully downloaded punkt_tab\n",
      "NLTK resources successfully verified\n",
      "Loading dataset...\n",
      "\n",
      "Processing training set...\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [15:27<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [00:01<00:00, 989.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing validation set...\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:53<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 1008.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test set...\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:52<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 1030.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete! Processed files saved as CSV.\n",
      "\n",
      "Dataset statistics:\n",
      "Training samples: 35773\n",
      "Validation samples: 2000\n",
      "Test samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# week 9: Cleaning and preprocessing the WikiSum dataset\n",
    "\n",
    "# importing necessary libraries\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import concurrent.futures\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "nltk.download('all') \n",
    "\n",
    "# Checking for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Ensuring the download directory exists\n",
    "    import os\n",
    "    nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "    if not os.path.exists(nltk_data_dir):\n",
    "        os.makedirs(nltk_data_dir)\n",
    "    \n",
    "    # Downloading required NLTK resources\n",
    "    resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'punkt_tab']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"Successfully downloaded {resource}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {resource}: {str(e)}\")\n",
    "    \n",
    "    # Verifying the downloads\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"NLTK resources successfully verified\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up NLTK resources: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Initializing BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class WikiSumDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for batch processing\"\"\"\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def batch_tokenize(texts: List[str], batch_size: int = 32) -> torch.Tensor:\n",
    "    \"\"\"Tokenize texts in batches using GPU\"\"\"\n",
    "    dataset = WikiSumDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_tokens = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            all_tokens.append(tokens)\n",
    "    \n",
    "    return all_tokens\n",
    "\n",
    "def load_and_prepare_wikisum():\n",
    "    \"\"\"Load WikiSum dataset and prepare initial dataframe\"\"\"\n",
    "    dataset = load_dataset(\"d0rj/wikisum\")\n",
    "    \n",
    "    # Converting to pandas DataFrame for easier manipulation\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing special characters, extra whitespace, etc.\"\"\"\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    \n",
    "    # Removing extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def calculate_info_density(tokens: List[str], stop_words: set) -> float:\n",
    "    \"\"\"Calculate information density using GPU-accelerated operations\"\"\"\n",
    "    # Converting tokens to tensor\n",
    "    token_tensor = torch.tensor([1 if token.lower() not in stop_words else 0 for token in tokens],\n",
    "                              dtype=torch.float32,\n",
    "                              device=device)\n",
    "    \n",
    "    # Calculating density\n",
    "    density = torch.mean(token_tensor).item()\n",
    "    return density\n",
    "\n",
    "def process_sentences_batch(sentences: List[str], stop_words: set) -> List[str]:\n",
    "    \"\"\"Process a batch of sentences in parallel\"\"\"\n",
    "    processed_sentences = []\n",
    "    \n",
    "    def process_single_sentence(sent: str) -> Tuple[bool, str]:\n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        # Skipping very short or very long sentences\n",
    "        if len(words) < 5 or len(words) > 100:\n",
    "            return False, sent\n",
    "        \n",
    "        # Calculating information density using GPU\n",
    "        info_density = calculate_info_density(words, stop_words)\n",
    "        \n",
    "        return info_density > 0.5, sent\n",
    "    \n",
    "    # Processing sentences in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda s: process_single_sentence(s), sentences))\n",
    "    \n",
    "    return [sent for keep, sent in results if keep]\n",
    "\n",
    "def preprocess_document(doc: str) -> str:\n",
    "    \"\"\"Preprocess a single document with GPU acceleration where possible\"\"\"\n",
    "    # Cleaning the text\n",
    "    clean_doc = clean_text(doc)\n",
    "    \n",
    "    # Splitting into sentences\n",
    "    sentences = sent_tokenize(clean_doc)\n",
    "    \n",
    "    # Processing sentences in batches\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = process_sentences_batch(sentences, stop_words)\n",
    "    \n",
    "    return ' '.join(processed_sentences)\n",
    "\n",
    "def process_dataset(df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:\n",
    "    \"\"\"Process the entire dataset with GPU acceleration\"\"\"\n",
    "    # Creating processing function for batch operations\n",
    "    def process_batch(texts: List[str]) -> List[str]:\n",
    "        return [preprocess_document(doc) for doc in texts]\n",
    "    \n",
    "    # Processing documents in batches\n",
    "    print(\"Processing documents...\")\n",
    "    n_batches = math.ceil(len(df) / batch_size)\n",
    "    processed_docs = []\n",
    "    \n",
    "    for i in tqdm(range(n_batches)):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_texts = df['article'].iloc[start_idx:end_idx].tolist()\n",
    "        \n",
    "        # Processing batch\n",
    "        processed_batch = process_batch(batch_texts)\n",
    "        processed_docs.extend(processed_batch)\n",
    "    \n",
    "    df['processed_document'] = processed_docs\n",
    "    \n",
    "    # Processing summaries similarly\n",
    "    print(\"Processing summaries...\")\n",
    "    processed_summaries = []\n",
    "    \n",
    "    for i in tqdm(range(n_batches)):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_texts = df['summary'].iloc[start_idx:end_idx].tolist()\n",
    "        \n",
    "        # Processing batch\n",
    "        processed_batch = [clean_text(text) for text in batch_texts]\n",
    "        processed_summaries.extend(processed_batch)\n",
    "    \n",
    "    df['processed_summary'] = processed_summaries\n",
    "    \n",
    "    # Removing empty entries\n",
    "    df = df[df['processed_document'].str.len() > 0]\n",
    "    df = df[df['processed_summary'].str.len() > 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the GPU-accelerated preprocessing pipeline\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    train_df, val_df, test_df = load_and_prepare_wikisum()\n",
    "    \n",
    "    # Processing each split\n",
    "    print(\"\\nProcessing training set...\")\n",
    "    processed_train = process_dataset(train_df)\n",
    "    \n",
    "    print(\"\\nProcessing validation set...\")\n",
    "    processed_val = process_dataset(val_df)\n",
    "    \n",
    "    print(\"\\nProcessing test set...\")\n",
    "    processed_test = process_dataset(test_df)\n",
    "    \n",
    "    # Saving processed datasets\n",
    "    processed_train.to_csv('processed_wikisum_train.csv', index=False)\n",
    "    processed_val.to_csv('processed_wikisum_val.csv', index=False)\n",
    "    processed_test.to_csv('processed_wikisum_test.csv', index=False)\n",
    "    \n",
    "    print(\"\\nPreprocessing complete! Processed files saved as CSV.\")\n",
    "    \n",
    "    # Printing some statistics\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(f\"Training samples: {len(processed_train)}\")\n",
    "    print(f\"Validation samples: {len(processed_val)}\")\n",
    "    print(f\"Test samples: {len(processed_test)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parmar.sa/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100-SXM4-40GB\n",
      "Loading data...\n",
      "\n",
      "Processing fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 scores:\n",
      "rouge1_f    0.380939\n",
      "rouge2_f    0.094451\n",
      "rougeL_f    0.193433\n",
      "dtype: float64\n",
      "\n",
      "Processing fold 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 scores:\n",
      "rouge1_f    0.381517\n",
      "rouge2_f    0.098991\n",
      "rougeL_f    0.199095\n",
      "dtype: float64\n",
      "\n",
      "Processing fold 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 scores:\n",
      "rouge1_f    0.377944\n",
      "rouge2_f    0.100161\n",
      "rougeL_f    0.196346\n",
      "dtype: float64\n",
      "\n",
      "Final Cross-validation Results:\n",
      "rouge1_f: 0.3801\n",
      "rouge2_f: 0.0979\n",
      "rougeL_f: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# week 10: Implementing the TextRank algorithm for extractive summarization and evaluating it\n",
    "\n",
    "# Importing necessary libraries\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name())\n",
    "\n",
    "class OptimizedTextRankSummarizer:\n",
    "    def __init__(self, bert_model_name: str = 'bert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Initialize the optimized TextRank summarizer\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.model = BertModel.from_pretrained(bert_model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def sentence_position_scores(self, num_sentences: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate position-based importance scores\n",
    "        \"\"\"\n",
    "        positions = np.arange(num_sentences)\n",
    "        # Exponential decay for position importance\n",
    "        position_scores = np.exp(-positions / num_sentences)\n",
    "        return position_scores\n",
    "\n",
    "    def calculate_sentence_lengths(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate normalized sentence lengths\n",
    "        \"\"\"\n",
    "        lengths = np.array([len(word_tokenize(sent)) for sent in sentences])\n",
    "        # Normalizing lengths to [0,1] range\n",
    "        return (lengths - lengths.min()) / (lengths.max() - lengths.min() + 1e-8)\n",
    "\n",
    "    def get_bert_embeddings(self, sentences: List[str]) -> torch.Tensor:\n",
    "        embeddings = []\n",
    "\n",
    "        # Moving model to GPU explicitly\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            inputs = self.tokenizer(sentence, return_tensors='pt',\n",
    "                                  padding=True, truncation=True,\n",
    "                                  max_length=512)\n",
    "            # Moving inputs to GPU\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                mean_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "                final_embedding = (cls_embedding + mean_embedding) / 2\n",
    "                # Moving result back to CPU before appending\n",
    "                embeddings.append(final_embedding.cpu())\n",
    "\n",
    "        return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "    def build_similarity_matrix(self, embeddings: torch.Tensor,\n",
    "                              position_scores: np.ndarray,\n",
    "                              length_scores: np.ndarray,\n",
    "                              threshold: float = 0.3) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build enhanced similarity matrix\n",
    "        \"\"\"\n",
    "        # Calculating cosine similarity\n",
    "        similarity_matrix = F.cosine_similarity(embeddings.unsqueeze(1),\n",
    "                                              embeddings.unsqueeze(0), dim=2)\n",
    "        similarity_matrix = similarity_matrix.numpy()\n",
    "\n",
    "        # Applying threshold\n",
    "        similarity_matrix[similarity_matrix < threshold] = 0\n",
    "\n",
    "        # Incorporating position and length importance\n",
    "        importance_scores = (position_scores + length_scores) / 2\n",
    "        importance_matrix = np.outer(importance_scores, importance_scores)\n",
    "        similarity_matrix = similarity_matrix * importance_matrix\n",
    "\n",
    "        # Normalizing\n",
    "        row_sums = similarity_matrix.sum(axis=1, keepdims=True)\n",
    "        similarity_matrix = np.divide(similarity_matrix, row_sums, where=row_sums!=0)\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def get_optimal_summary_length(self, text_length: int, num_sentences: int) -> int:\n",
    "        \"\"\"\n",
    "        Dynamic summary length based on text properties\n",
    "        \"\"\"\n",
    "        # Base length based on text length\n",
    "        if text_length < 500:\n",
    "            base_length = 3\n",
    "        elif text_length < 1000:\n",
    "            base_length = 4\n",
    "        elif text_length < 2000:\n",
    "            base_length = 5\n",
    "        else:\n",
    "            base_length = 6\n",
    "\n",
    "        # Adjusting based on number of sentences\n",
    "        return min(base_length, max(3, num_sentences // 4))\n",
    "\n",
    "    def summarize(self, text: str, num_sentences: int = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate extractive summary using optimized TextRank\n",
    "        \"\"\"\n",
    "        # Splitting and preprocess\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) < 3:\n",
    "            return text\n",
    "\n",
    "        # Calculating optimal summary length\n",
    "        if num_sentences is None:\n",
    "            num_sentences = self.get_optimal_summary_length(len(text), len(sentences))\n",
    "\n",
    "        # Calculating importance scores\n",
    "        position_scores = self.sentence_position_scores(len(sentences))\n",
    "        length_scores = self.calculate_sentence_lengths(sentences)\n",
    "\n",
    "        # Generating embeddings and similarity matrix\n",
    "        embeddings = self.get_bert_embeddings(sentences)\n",
    "        similarity_matrix = self.build_similarity_matrix(embeddings,\n",
    "                                                       position_scores,\n",
    "                                                       length_scores)\n",
    "\n",
    "        # Running PageRank with personalization\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        personalization = dict(enumerate(position_scores))\n",
    "        scores = nx.pagerank(nx_graph,\n",
    "                           alpha=0.85,\n",
    "                           personalization=personalization,\n",
    "                           max_iter=100,\n",
    "                           tol=1e-6)\n",
    "\n",
    "        # Selecting sentences\n",
    "        sentence_scores = np.array([scores[i] for i in range(len(scores))])\n",
    "        top_idx = np.argsort(sentence_scores)[-num_sentences:]\n",
    "        top_idx = sorted(top_idx)\n",
    "\n",
    "        # Combining sentences\n",
    "        summary = ' '.join([sentences[i] for i in top_idx])\n",
    "\n",
    "        return summary\n",
    "\n",
    "def evaluate_with_cross_validation(val_path: str = 'processed_wikisum_val.csv',\n",
    "                                 n_folds: int = 3,\n",
    "                                 samples_per_fold: int = 100):\n",
    "    \"\"\"\n",
    "    Evaluate with cross-validation for more robust results\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    processed_val = pd.read_csv(val_path)\n",
    "\n",
    "    # Initializing summarizer and scorer\n",
    "    summarizer = OptimizedTextRankSummarizer()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'],\n",
    "                                    use_stemmer=True)\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    # Cross-validation\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\nProcessing fold {fold + 1}/{n_folds}\")\n",
    "        start_idx = fold * samples_per_fold\n",
    "        end_idx = start_idx + samples_per_fold\n",
    "        fold_samples = processed_val.iloc[start_idx:end_idx]\n",
    "\n",
    "        fold_results = []\n",
    "        for _, row in tqdm(fold_samples.iterrows(), total=len(fold_samples)):\n",
    "            generated_summary = summarizer.summarize(row['processed_document'])\n",
    "            scores = scorer.score(row['processed_summary'], generated_summary)\n",
    "\n",
    "            fold_results.append({\n",
    "                'rouge1_f': scores['rouge1'].fmeasure,\n",
    "                'rouge2_f': scores['rouge2'].fmeasure,\n",
    "                'rougeL_f': scores['rougeL'].fmeasure\n",
    "            })\n",
    "\n",
    "        all_scores.append(pd.DataFrame(fold_results).mean())\n",
    "        print(f\"Fold {fold + 1} scores:\")\n",
    "        print(pd.DataFrame(fold_results).mean())\n",
    "\n",
    "    # Calculating and printing final results\n",
    "    final_scores = pd.DataFrame(all_scores).mean()\n",
    "    print(\"\\nFinal Cross-validation Results:\")\n",
    "    for metric, score in final_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_with_cross_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parmar.sa/.venv/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 19:01, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parmar.sa/.venv/lib64/python3.9/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary with base model:\n",
      "Generated Summary: Figure out how you typically spend your time. Before you can figure out how to optimize your time, get a good sense for how you already manage day-to-day affairs. If you have to attend school or work, these hours are already managed for you. In your free time, you have a much greater degree of flexibility. Spend a few days keeping track of how you do every day. Pay close attention to how you spend free time. Do you play video games, or do you clean the house? Make a list of these activities and how long you spend on them. Estimate how long in your daily schedule. It's likely that you spend a relatively large amount of some of your days simply traveling to or from work\n",
      "\n",
      "Summary with fine-tuned model:\n",
      "Generated Summary: To optimize your time, start by making a list of how long you spend commuting for school, work, and errands. Then, determine how much time you'll spend on these activities. For example, if you have to go to school or work, make sure that you spend all of your time working on your goals. If you don't have time to do these activities, try to keep up with your work schedule so that you can focus on what you want to accomplish. Once you've done these tasks, you'll be able to focus on your work, which will help you achieve your goals more effectively. You'll also want to make sure you have enough time to spend with your family, friends, and family, so you\n",
      "\n",
      "Contents of ./bart_wikisum directory:\n",
      "['checkpoint-21', 'checkpoint-30', 'checkpoint-6', 'checkpoint-75', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "# week 11: Fine-tuning the BART model for abstractive summarization\n",
    "import torch\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from typing import Dict\n",
    "import gc\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = load_dataset(\"d0rj/wikisum\", streaming=True)\n",
    "\n",
    "\n",
    "# Function for Memory management\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "class WikiSumDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=384):\n",
    "        self.dataset = list(dataset.take(1000))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            item['article'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            targets = self.tokenizer(\n",
    "                item['summary'],\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "def train_bart_model():\n",
    "    # Setting memory efficient attention\n",
    "    torch.backends.cuda.max_memory_allocated = lambda: 0\n",
    "\n",
    "    # Loading data\n",
    "    dataset = load_dataset(\"d0rj/wikisum\", streaming=True)\n",
    "    train_data = dataset['train'].take(100000) \n",
    "    val_data = dataset['validation'].take(10000)\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    # Initializing tokenizer and saving it immediately\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "    os.makedirs(\"./bart_wikisum\", exist_ok=True)\n",
    "    tokenizer.save_pretrained(\"./bart_wikisum\")\n",
    "\n",
    "    train_dataset = WikiSumDataset(train_data, tokenizer)\n",
    "    val_dataset = WikiSumDataset(val_data, tokenizer)\n",
    "\n",
    "    # Model initialization\n",
    "    model = BartForConditionalGeneration.from_pretrained(\n",
    "        'facebook/bart-base',\n",
    "        gradient_checkpointing=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    clear_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_wikisum\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    fp16_backend=\"auto\",\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    dataloader_num_workers=0,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    remove_unused_columns=True,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.02,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda x: compute_metrics(x, tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    return trainer, tokenizer\n",
    "\n",
    "def generate_summary(text, model_path):\n",
    "    try:\n",
    "        # Loading tokenizer and model from the same directory\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "        # Moving model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Moving inputs to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            num_beams=4,\n",
    "            length_penalty=0.8,\n",
    "            no_repeat_ngram_size=3,\n",
    "            min_length=30,\n",
    "            max_length=150,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,  # Adding some randomness\n",
    "            temperature=0.7  # Controlling randomness\n",
    "        )\n",
    "\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summary generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clearing GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Training model\n",
    "    trainer, tokenizer = train_bart_model()\n",
    "    trainer.train()\n",
    "\n",
    "    # Saving both model and tokenizer\n",
    "    trainer.save_model(\"./bart_wikisum\")\n",
    "    tokenizer.save_pretrained(\"./bart_wikisum\")\n",
    "\n",
    "    # Getting first example from the validation set\n",
    "    example = next(iter(dataset['validation']))\n",
    "    text = example['article']\n",
    "\n",
    "    print(\"Summary with base model:\")\n",
    "    summary_base = generate_summary(text, \"facebook/bart-base\")\n",
    "    print(f\"Generated Summary: {summary_base}\")\n",
    "\n",
    "    print(\"\\nSummary with fine-tuned model:\")\n",
    "    summary_finetuned = generate_summary(text, \"./bart_wikisum\")\n",
    "    print(f\"Generated Summary: {summary_finetuned}\")\n",
    "\n",
    "    # Printing directory contents for debugging\n",
    "    print(\"\\nContents of ./bart_wikisum directory:\")\n",
    "    print(os.listdir(\"./bart_wikisum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/parmar.sa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.95k/7.95k [00:00<00:00, 5.86MB/s]\n",
      "100%|██████████| 50/50 [06:07<00:00,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "\n",
      "Hybrid Method:\n",
      "rouge1: 0.3952\n",
      "bertscore: 0.6026\n",
      "meteor: 0.2890\n",
      "\n",
      "Extractive Method:\n",
      "rouge1: 0.3964\n",
      "bertscore: 0.5999\n",
      "meteor: 0.3028\n",
      "\n",
      "Abstractive Method:\n",
      "rouge1: 0.4242\n",
      "bertscore: 0.6144\n",
      "meteor: 0.3003\n",
      "\n",
      "Method Selection Distribution:\n",
      "method_used\n",
      "extractive     0.64\n",
      "hybrid         0.28\n",
      "abstractive    0.08\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Results saved to hybrid_summarization_results.csv and summarization_results.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# week 12: Implementing the hybrid approach and evaluating its performance using ROUGE, BERTScore, and METEOR with analysis and visualization of the results\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List, Dict, Union\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import gc\n",
    "import evaluate\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class HybridSummarizer:\n",
    "    \"\"\"\n",
    "    A hybrid summarization system that combines extractive (TextRank) and \n",
    "    abstractive (BART) approaches with adaptive selection and comprehensive metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, bart_model_path: str = \"./bart_wikisum\"):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid summarizer with both extractive and abstractive components\n",
    "        and evaluation metrics\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Initializing BART components\n",
    "        try:\n",
    "            self.tokenizer = BartTokenizer.from_pretrained(bart_model_path)\n",
    "            self.bart_model = BartForConditionalGeneration.from_pretrained(bart_model_path).to(self.device)\n",
    "        except:\n",
    "            print(\"Using base BART model as fallback...\")\n",
    "            self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "            self.bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(self.device)\n",
    "        \n",
    "        # Initializing TextRank component\n",
    "        self.textrank_summarizer = OptimizedTextRankSummarizer()\n",
    "        \n",
    "        # Initializing all evaluation metrics\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.bertscore = evaluate.load('bertscore')\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Memory management utility\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def calculate_bertscore(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Calculate BERTScore\"\"\"\n",
    "        results = self.bertscore.compute(\n",
    "            predictions=[candidate],\n",
    "            references=[reference],\n",
    "            lang=\"en\",\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\"\n",
    "        )\n",
    "        return sum(results['f1']) / len(results['f1'])\n",
    "\n",
    "    def calculate_meteor(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Calculate METEOR score\"\"\"\n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        candidate_tokens = nltk.word_tokenize(candidate)\n",
    "        return meteor_score([reference_tokens], candidate_tokens)\n",
    "\n",
    "    def generate_abstractive_summary(self, text: str) -> str:\n",
    "        \"\"\"Generate abstractive summary using fine-tuned BART\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        summary_ids = self.bart_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            num_beams=4,\n",
    "            length_penalty=0.8,\n",
    "            no_repeat_ngram_size=3,\n",
    "            min_length=30,\n",
    "            max_length=150,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def adaptive_summarize(self, text: str, strategy: str = 'hybrid') -> Dict[str, str]:\n",
    "        \"\"\"Generate summary using specified strategy with adaptive selection\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Generating summaries using both methods\n",
    "        extractive_summary = self.textrank_summarizer.summarize(text)\n",
    "        abstractive_summary = self.generate_abstractive_summary(text)\n",
    "        \n",
    "        # Calculating comprehensive metrics for both summaries\n",
    "        extractive_scores = self.rouge_scorer.score(text[:200], extractive_summary)\n",
    "        abstractive_scores = self.rouge_scorer.score(text[:200], abstractive_summary)\n",
    "        \n",
    "        ext_rouge1 = extractive_scores['rouge1'].fmeasure\n",
    "        abs_rouge1 = abstractive_scores['rouge1'].fmeasure\n",
    "        \n",
    "        # Calculating BERTScore and METEOR for both summaries\n",
    "        ext_bertscore = self.calculate_bertscore(text[:200], extractive_summary)\n",
    "        abs_bertscore = self.calculate_bertscore(text[:200], abstractive_summary)\n",
    "        ext_meteor = self.calculate_meteor(text[:200], extractive_summary)\n",
    "        abs_meteor = self.calculate_meteor(text[:200], abstractive_summary)\n",
    "        \n",
    "        if strategy == 'hybrid':\n",
    "            # Adaptive selection using all metrics\n",
    "            ext_score = (ext_rouge1 + ext_bertscore + ext_meteor) / 3\n",
    "            abs_score = (abs_rouge1 + abs_bertscore + abs_meteor) / 3\n",
    "            \n",
    "            if abs_score > ext_score * 1.1:\n",
    "                final_summary = abstractive_summary\n",
    "                method_used = 'abstractive'\n",
    "            elif ext_score > abs_score * 1.1:\n",
    "                final_summary = extractive_summary\n",
    "                method_used = 'extractive'\n",
    "            else:\n",
    "                # Weighted combination based on comprehensive scores\n",
    "                total_score = ext_score + abs_score\n",
    "                ext_weight = ext_score / total_score\n",
    "                abs_weight = abs_score / total_score\n",
    "                \n",
    "                ext_sents = sent_tokenize(extractive_summary)\n",
    "                abs_sents = sent_tokenize(abstractive_summary)\n",
    "                \n",
    "                combined_sents = []\n",
    "                for i in range(min(len(ext_sents), len(abs_sents))):\n",
    "                    if np.random.random() < ext_weight:\n",
    "                        combined_sents.append(ext_sents[i])\n",
    "                    else:\n",
    "                        combined_sents.append(abs_sents[i])\n",
    "                \n",
    "                final_summary = ' '.join(combined_sents)\n",
    "                method_used = 'hybrid'\n",
    "        else:\n",
    "            final_summary = extractive_summary if strategy == 'extractive' else abstractive_summary\n",
    "            method_used = strategy\n",
    "            \n",
    "        results = {\n",
    "            'summary': final_summary,\n",
    "            'method_used': method_used,\n",
    "            'metrics': {\n",
    "                'rouge1': ext_rouge1 if method_used == 'extractive' else abs_rouge1,\n",
    "                'bertscore': ext_bertscore if method_used == 'extractive' else abs_bertscore,\n",
    "                'meteor': ext_meteor if method_used == 'extractive' else abs_meteor\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_hybrid_system(model_path: str = \"./bart_wikisum\", num_samples: int = 100):\n",
    "    \"\"\"Evaluate the hybrid summarization system on the WikiSum dataset with comprehensive metrics\"\"\"\n",
    "    dataset = load_dataset(\"d0rj/wikisum\")\n",
    "    eval_data = dataset['validation'].select(range(num_samples))\n",
    "    \n",
    "    summarizer = HybridSummarizer(model_path)\n",
    "    all_metrics = []\n",
    "    \n",
    "    for item in tqdm(eval_data):\n",
    "        try:\n",
    "            # Generating summaries using different strategies\n",
    "            hybrid_result = summarizer.adaptive_summarize(item['article'], strategy='hybrid')\n",
    "            extractive_result = summarizer.adaptive_summarize(item['article'], strategy='extractive')\n",
    "            abstractive_result = summarizer.adaptive_summarize(item['article'], strategy='abstractive')\n",
    "            \n",
    "            # Calculating all metrics for each method\n",
    "            metrics = {\n",
    "                'hybrid_rouge1': summarizer.rouge_scorer.score(item['summary'], \n",
    "                    hybrid_result['summary'])['rouge1'].fmeasure,\n",
    "                'hybrid_bertscore': summarizer.calculate_bertscore(item['summary'], \n",
    "                    hybrid_result['summary']),\n",
    "                'hybrid_meteor': summarizer.calculate_meteor(item['summary'], \n",
    "                    hybrid_result['summary']),\n",
    "                \n",
    "                'extractive_rouge1': summarizer.rouge_scorer.score(item['summary'], \n",
    "                    extractive_result['summary'])['rouge1'].fmeasure,\n",
    "                'extractive_bertscore': summarizer.calculate_bertscore(item['summary'], \n",
    "                    extractive_result['summary']),\n",
    "                'extractive_meteor': summarizer.calculate_meteor(item['summary'], \n",
    "                    extractive_result['summary']),\n",
    "                \n",
    "                'abstractive_rouge1': summarizer.rouge_scorer.score(item['summary'], \n",
    "                    abstractive_result['summary'])['rouge1'].fmeasure,\n",
    "                'abstractive_bertscore': summarizer.calculate_bertscore(item['summary'], \n",
    "                    abstractive_result['summary']),\n",
    "                'abstractive_meteor': summarizer.calculate_meteor(item['summary'], \n",
    "                    abstractive_result['summary']),\n",
    "                \n",
    "                'method_used': hybrid_result['method_used']\n",
    "            }\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        if len(all_metrics) % 10 == 0:\n",
    "            summarizer.clear_memory()\n",
    "    \n",
    "    # Calculating and displaying final results\n",
    "    results_df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    \n",
    "    # Printing average scores for each method and metric\n",
    "    for method in ['hybrid', 'extractive', 'abstractive']:\n",
    "        print(f\"\\n{method.capitalize()} Method:\")\n",
    "        for metric in ['rouge1', 'bertscore', 'meteor']:\n",
    "            score = results_df[f'{method}_{metric}'].mean()\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\nMethod Selection Distribution:\")\n",
    "    print(results_df['method_used'].value_counts(normalize=True))\n",
    "    \n",
    "    # Saving detailed results\n",
    "    results_df.to_csv(\"hybrid_summarization_results.csv\", index=False)\n",
    "    \n",
    "    # Creating visualization of results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    metrics = ['rouge1', 'bertscore', 'meteor']\n",
    "    methods = ['hybrid', 'extractive', 'abstractive']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        scores = [results_df[f'{method}_{metric}'].mean() for metric in metrics]\n",
    "        ax.bar(x + i*width, scores, width, label=method.capitalize())\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Comparison of Summarization Methods')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(['ROUGE-1', 'BERTScore', 'METEOR'])\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('summarization_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing evaluation...\")\n",
    "    results = evaluate_hybrid_system(num_samples=50)\n",
    "    print(\"\\nResults saved to hybrid_summarization_results.csv and summarization_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
